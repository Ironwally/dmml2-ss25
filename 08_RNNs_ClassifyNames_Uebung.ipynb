{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN for Classifying Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are building and training a basic character-level RNN to classify\n",
    "words. A character-level RNN reads words as a series of characters -\n",
    "outputting a prediction and \"hidden state\" at each step, feeding its\n",
    "previous hidden state into each next time step. We take the final prediction\n",
    "to be the output, i.e. which class the word belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data\n",
    "\n",
    "Download the data in folder `data/names` from GitHub.\n",
    "\n",
    "Included in the ``data/names`` directory are 18 text files named as\n",
    "``[Language].txt``. Each file contains a bunch of names, one name per\n",
    "line, mostly romanized (but we still need to convert from Unicode to\n",
    "ASCII)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get all the filenames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/names/Korean.txt', 'data/names/Portuguese.txt', 'data/names/Dutch.txt', 'data/names/Italian.txt', 'data/names/French.txt', 'data/names/Vietnamese.txt', 'data/names/Chinese.txt', 'data/names/Irish.txt', 'data/names/Japanese.txt', 'data/names/Scottish.txt', 'data/names/Greek.txt', 'data/names/Czech.txt', 'data/names/Russian.txt', 'data/names/English.txt', 'data/names/Spanish.txt', 'data/names/German.txt', 'data/names/Arabic.txt', 'data/names/Polish.txt']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "filenames = glob.glob('data/names/*.txt')\n",
    "\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save each language as a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Korean', 'Portuguese', 'Dutch', 'Italian', 'French', 'Vietnamese', 'Chinese', 'Irish', 'Japanese', 'Scottish', 'Greek', 'Czech', 'Russian', 'English', 'Spanish', 'German', 'Arabic', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "all_categories = []\n",
    "\n",
    "\n",
    "for filename in filenames:\n",
    "    language = os.path.splitext(os.path.basename(filename))[0]\n",
    "    all_categories.append(language)\n",
    "\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data and put every name in a list together and its category (=label) in a second list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 20074)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "\n",
    "for index, filename in enumerate(filenames):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    category = all_categories[index]\n",
    "    for line in lines:\n",
    "        X.append(line)\n",
    "        y.append(category)\n",
    "\n",
    "n_categories = len(all_categories)\n",
    "n_categories, len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check which characters are included in the names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ą', 'ú', 'H', ':', 'C', 'ù', 'r', ' ', 'ż', 'õ', 'ò', '/', 'á', 'j', 'f', ',', '-', 'ó', 'k', 'ü', 'ì', 'E', 'w', 'B', 'l', '1', 'ß', 'L', 'F', 'K', 'M', 'ä', 'I', 'ń', 'z', 'y', 'p', 'R', 'Q', 'J', 'n', 'ł', 'P', 'i', 'q', 'u', 'Ś', '\\xa0', 'b', 'U', 'g', 'N', 'a', 'ç', 'ö', 't', 'x', 'ê', 'V', 'A', 'ñ', 'G', 'D', 'à', 'ã', 'Á', 'T', 'è', 'c', 'h', 'é', 'Y', 'v', 'W', 'o', \"'\", 'S', 'd', 'Z', 'É', 'í', 's', 'm', 'e', 'Ż', 'X', 'O'}\n",
      "87 characters\n"
     ]
    }
   ],
   "source": [
    "all_characters = set([c for name in X for c in name])\n",
    "print(all_characters)\n",
    "print(len(all_characters), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the files contain many special characters that make our problem more difficult. To reduce the character count, we only allow ASCII symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab is of size 52 and contains: abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# these is the vocabulary we will use\n",
    "all_letters = string.ascii_letters\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "print(f\"Vocab is of size {n_letters} and contains:\", all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slusarski\n",
      "Fruhling\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "# this method converts anything into ascii\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "print(unicodeToAscii('Ślusàrski'))\n",
    "print(unicodeToAscii('Frühling'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z', 'G', 'f', 'D', 'y', 'p', 'R', 'Q', 'T', 'J', 'H', 'c', 'k', 'n', 'P', 'h', 'V', 'C', 'E', 'w', 'B', 'O', 'i', 'l', 'Y', 'q', 'v', 'r', 'u', 'W', 'o', 'b', 'L', 'U', 'g', 'N', 'a', 'S', 'F', 'K', 'd', 'Z', 'M', 't', 's', 'm', 'e', 'I', 'x', 'X', 'j', 'A'}\n",
      "52 characters\n"
     ]
    }
   ],
   "source": [
    "# convert all letters to ascii\n",
    "X = [unicodeToAscii(x) for x in X]\n",
    "\n",
    "# print again all characters\n",
    "all_characters = set([c for name in X for c in name])\n",
    "print(all_characters)\n",
    "print(len(all_characters), \"characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we successfully reduced the number of characters and can now divide the data into train and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data points: 16059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Train data points:\", len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning Names into Tensors\n",
    "--------------------------\n",
    "\n",
    "Now that we have all the names organized, we need to turn them into\n",
    "Tensors to make any use of them.\n",
    "\n",
    "To represent a single letter, we use a \"one-hot vector\" of size\n",
    "``<1 x n_letters>``. A one-hot vector is filled with 0s except for a 1\n",
    "at index of the current letter, e.g. ``\"b\" = <0 1 0 0 0 ...>``.\n",
    "\n",
    "To make a word we join a bunch of those into a 2D matrix\n",
    "``<line_length x 1 x n_letters>``.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes everything is in\n",
    "batches - we're just using a batch size of 1 here.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    index = all_letters.find(letter)\n",
    "    tensor[0][index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Know lets check how the encoding of one letter looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(letterToTensor('J'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the label into a number, which is just the index of the category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categoryToTensor(category):\n",
    "    index = all_categories.index(category)\n",
    "    return torch.tensor([index], dtype=torch.long)\n",
    "\n",
    "categoryToTensor(\"Korean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the RNN\n",
    "====================\n",
    "\n",
    "This RNN module has two linear layers. One calculates the next hidden state, the other one the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = 128 # number of hidden layer size\n",
    "\n",
    "        self.input2hidden = nn.Linear(input_size + self.hidden_size, self.hidden_size)\n",
    "        self.input2output = nn.Linear(input_size + self.hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        combined = torch.cat((x, hidden), 1) # input and hidden state are combined\n",
    "        hidden = self.input2hidden(combined) # calculate next hidden state\n",
    "        output = self.input2output(combined) # calculate output state\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a step of this network we need to pass an input (in our case, the\n",
    "tensor for the current letter) and a previous hidden state (which we\n",
    "initialize as zeros at first). We get back the output and a next hidden state (which we keep for the next\n",
    "step).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0555, 0.0561, 0.0533, 0.0539, 0.0538, 0.0594, 0.0599, 0.0544, 0.0543,\n",
      "         0.0576, 0.0609, 0.0600, 0.0529, 0.0565, 0.0519, 0.0517, 0.0521, 0.0555]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(n_letters, n_categories)\n",
    "\n",
    "x = letterToTensor('A')\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "output, next_hidden = rnn(x, hidden)\n",
    "print(torch.softmax(output, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the output is a ``<1 x n_categories>`` Tensor, where\n",
    "every item is the likelihood of that category (higher is more likely).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Training the Network\n",
    "--------------------\n",
    "\n",
    "Finish the following training function to train the RNN on the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1\n",
      "[2,  2000] loss: 1.876466891\n",
      "[2,  4000] loss: 1.563148097\n",
      "[2,  6000] loss: 1.482917607\n",
      "[2,  8000] loss: 1.382190724\n",
      "[2, 10000] loss: 1.372809337\n",
      "[2, 12000] loss: 1.297917074\n",
      "[2, 14000] loss: 1.298332493\n",
      "[2, 16000] loss: 1.212345366\n",
      "Training epoch: 2\n",
      "[3,  2000] loss: 1.200130932\n",
      "[3,  4000] loss: 1.151070391\n",
      "[3,  6000] loss: 1.130098969\n",
      "[3,  8000] loss: 1.091652181\n",
      "[3, 10000] loss: 1.099722245\n",
      "[3, 12000] loss: 1.093102953\n",
      "[3, 14000] loss: 1.118540812\n",
      "[3, 16000] loss: 1.050824200\n",
      "Training epoch: 3\n",
      "[4,  2000] loss: 1.074618266\n",
      "[4,  4000] loss: 1.041089648\n",
      "[4,  6000] loss: 1.002159093\n",
      "[4,  8000] loss: 0.985734719\n",
      "[4, 10000] loss: 0.986821996\n",
      "[4, 12000] loss: 1.006558733\n",
      "[4, 14000] loss: 1.037499289\n",
      "[4, 16000] loss: 0.966549328\n",
      "Training epoch: 4\n",
      "[5,  2000] loss: 1.005560512\n",
      "[5,  4000] loss: 0.981200496\n",
      "[5,  6000] loss: 0.933879589\n",
      "[5,  8000] loss: 0.923400705\n",
      "[5, 10000] loss: 0.922129239\n",
      "[5, 12000] loss: 0.954495491\n",
      "[5, 14000] loss: 0.982984848\n",
      "[5, 16000] loss: 0.913223639\n",
      "Training epoch: 5\n",
      "[6,  2000] loss: 0.961948617\n",
      "[6,  4000] loss: 0.942794579\n",
      "[6,  6000] loss: 0.892738262\n",
      "[6,  8000] loss: 0.886208058\n",
      "[6, 10000] loss: 0.884220646\n",
      "[6, 12000] loss: 0.919252559\n",
      "[6, 14000] loss: 0.947662172\n",
      "[6, 16000] loss: 0.877383838\n",
      "Training epoch: 6\n",
      "[7,  2000] loss: 0.931224711\n",
      "[7,  4000] loss: 0.915008899\n",
      "[7,  6000] loss: 0.865719111\n",
      "[7,  8000] loss: 0.859685317\n",
      "[7, 10000] loss: 0.858496778\n",
      "[7, 12000] loss: 0.893131949\n",
      "[7, 14000] loss: 0.921059415\n",
      "[7, 16000] loss: 0.853246224\n",
      "Training epoch: 7\n",
      "[8,  2000] loss: 0.907585050\n",
      "[8,  4000] loss: 0.893011156\n",
      "[8,  6000] loss: 0.846371569\n",
      "[8,  8000] loss: 0.838730584\n",
      "[8, 10000] loss: 0.839348632\n",
      "[8, 12000] loss: 0.873430032\n",
      "[8, 14000] loss: 0.900254398\n",
      "[8, 16000] loss: 0.835308350\n",
      "Training epoch: 8\n",
      "[9,  2000] loss: 0.888990087\n",
      "[9,  4000] loss: 0.874066123\n",
      "[9,  6000] loss: 0.831128714\n",
      "[9,  8000] loss: 0.821294689\n",
      "[9, 10000] loss: 0.823669839\n",
      "[9, 12000] loss: 0.857898915\n",
      "[9, 14000] loss: 0.884553586\n",
      "[9, 16000] loss: 0.820540545\n",
      "Training epoch: 9\n",
      "[10,  2000] loss: 0.873851521\n",
      "[10,  4000] loss: 0.857044391\n",
      "[10,  6000] loss: 0.818359652\n",
      "[10,  8000] loss: 0.807057296\n",
      "[10, 10000] loss: 0.811561608\n",
      "[10, 12000] loss: 0.844369830\n",
      "[10, 14000] loss: 0.872641975\n",
      "[10, 16000] loss: 0.808375752\n",
      "Finished Learning\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "rnn.to(device)\n",
    "rnn.train()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    running_loss = 0.0\n",
    "    print(\"Training epoch:\", epoch)\n",
    "    # iterate through all names in X_train\n",
    "    for i, name in enumerate(X_train, 0):\n",
    "        next_hidden = rnn.initHidden()   \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for char in name:\n",
    "            x = letterToTensor(char)\n",
    "            output, next_hidden = rnn(x, next_hidden)\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        category = y_train[i]\n",
    "        loss = criterion(output, categoryToTensor(category))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.9f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print(\"Finished Learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Evaluating the Results\n",
    "\n",
    "Evaluate the accuarcy of the RNN on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 74.17185554171856\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rnn.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "correct = 0\n",
    "for i, name in enumerate(X_test, 0):\n",
    "    next_hidden = rnn.initHidden() \n",
    "    for char in name:\n",
    "        x = letterToTensor(char)\n",
    "        output, next_hidden = rnn(x, next_hidden)\n",
    "    predicted_category = all_categories[output.argmax().item()]\n",
    "    #prediction = torch.argmax(output, 1).item()\n",
    "    #category_index = all_categories[prediction]\n",
    "    all_preds+=predicted_category\n",
    "    category = y_test[i]\n",
    "    all_labels+=category\n",
    "    if predicted_category == category:\n",
    "            correct+=1\n",
    "\n",
    "accuracy = correct / len(X_test)\n",
    "\n",
    "print(f\"Accuracy is: {100. * accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Running on User Input\n",
    "\n",
    "Write a function that takes an abritrary name as input and outputs the top 3 categories of the RNN for the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take arbitrary name as input and output top 3 categories\n",
    "def name_heritage_specifier(model, name):\n",
    "    model.eval\n",
    "    next_hidden = rnn.initHidden() \n",
    "    for char in name:\n",
    "        x = letterToTensor(char)\n",
    "        output, next_hidden = rnn(x, next_hidden)\n",
    "    output = torch.softmax(output, 0)\n",
    "    return list(zip(all_categories, output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Korean', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Portuguese', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Dutch', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Italian', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('French', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Vietnamese', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Chinese', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Irish', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Japanese', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Scottish', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Greek', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Czech', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Russian', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('English', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Spanish', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('German', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Arabic', tensor(1., grad_fn=<UnbindBackward0>)),\n",
       " ('Polish', tensor(1., grad_fn=<UnbindBackward0>))]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_heritage_specifier(rnn, \"Colin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmml2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
